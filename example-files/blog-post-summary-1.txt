We advanced our project in a number of ways this week. One thing we did was figure out how to make HTTP requests from the Magic Leap headset. To demonstrate this, we put together a simple demo that makes repeated requests to a webpage that serves the current time, extracts that time from the response, and displays it in the scene. Additionally, we implemented the interactive UI windows. In our current implementation, these windows can load images and respond to controller interactions (e.g., clicking and dragging) based on raycasting. We also started investigating the world reconstruction API that provides functionality for detecting planar surfaces in the physical world, as well as reference documentation related to the controller and persistent storage functions. The reason we're looking into leveraging the persistent storage functions is so workspaces can persist across sessions. In order to experiment with 3D drawing before moving onto 2D drawing for annotations, we experimented with using trigger events from the controller to dynamically create 3D spheres whenever the controller trigger is pressed. Finally, we started working on text file display.